{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(25000, 2)\n",
      "(50000, 2)\n",
      "25000\n",
      "25000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(unlabeled_train.shape)\n",
    "\n",
    "print(train['review'].size)\n",
    "print(test['review'].size)\n",
    "print(unlabeled_train['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'all', 'this', 'stuff', 'go', 'down', 'at', 'the', 'moment', 'with']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Word2VecUtil import Word2VecUtil\n",
    "Word2VecUtil.review_to_wordlist(train['review'][0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LGPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in train['review']:\n",
    "    sentences += Word2VecUtil.review_to_sentences(\n",
    "    review, remove_stopwords=False) #stopwords는 불용어라는 뜻 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for review in unlabeled_train['review']:\n",
    "    sentences += Word2VecUtil.review_to_sentences(\n",
    "    review, remove_stopwords=False)\n",
    "    \n",
    "len(sentences)\n",
    "\n",
    "# 단어의 개수가 아닌, 문장의 개수를 센다. like가 긍정인지 부정인지 알기 위해서는 앞뒤 문맥을 알아야하기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'all', 'this', 'stuff', 'go', 'down', 'at', 'the', 'moment', 'with']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s : %(message)s',\n",
    "                   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-30 11:17:36,478: INFO : collecting all words and their counts\n",
      "2020-04-30 11:17:36,501: INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-04-30 11:17:37,080: INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 12465 word types\n",
      "2020-04-30 11:17:37,314: INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 17070 word types\n",
      "2020-04-30 11:17:37,476: INFO : PROGRESS: at sentence #30000, processed 671314 words, keeping 20370 word types\n",
      "2020-04-30 11:17:37,711: INFO : PROGRESS: at sentence #40000, processed 897814 words, keeping 23125 word types\n",
      "2020-04-30 11:17:37,941: INFO : PROGRESS: at sentence #50000, processed 1116962 words, keeping 25365 word types\n",
      "2020-04-30 11:17:38,186: INFO : PROGRESS: at sentence #60000, processed 1338403 words, keeping 27283 word types\n",
      "2020-04-30 11:17:38,465: INFO : PROGRESS: at sentence #70000, processed 1561579 words, keeping 29024 word types\n",
      "2020-04-30 11:17:38,970: INFO : PROGRESS: at sentence #80000, processed 1780886 words, keeping 30603 word types\n",
      "2020-04-30 11:17:39,262: INFO : PROGRESS: at sentence #90000, processed 2004995 words, keeping 32223 word types\n",
      "2020-04-30 11:17:39,496: INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 33579 word types\n",
      "2020-04-30 11:17:39,727: INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 34827 word types\n",
      "2020-04-30 11:17:39,947: INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 36183 word types\n",
      "2020-04-30 11:17:40,180: INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 37353 word types\n",
      "2020-04-30 11:17:40,363: INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 38376 word types\n",
      "2020-04-30 11:17:40,569: INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 39556 word types\n",
      "2020-04-30 11:17:40,808: INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 40629 word types\n",
      "2020-04-30 11:17:41,074: INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 41628 word types\n",
      "2020-04-30 11:17:41,310: INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 42599 word types\n",
      "2020-04-30 11:17:41,506: INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 43461 word types\n",
      "2020-04-30 11:17:41,734: INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 44301 word types\n",
      "2020-04-30 11:17:41,954: INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 45212 word types\n",
      "2020-04-30 11:17:42,177: INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 46134 word types\n",
      "2020-04-30 11:17:42,492: INFO : PROGRESS: at sentence #230000, processed 5117546 words, keeping 46986 word types\n",
      "2020-04-30 11:17:43,307: INFO : PROGRESS: at sentence #240000, processed 5345051 words, keeping 47854 word types\n",
      "2020-04-30 11:17:43,669: INFO : PROGRESS: at sentence #250000, processed 5559166 words, keeping 48699 word types\n",
      "2020-04-30 11:17:43,954: INFO : PROGRESS: at sentence #260000, processed 5779147 words, keeping 49469 word types\n",
      "2020-04-30 11:17:44,109: INFO : PROGRESS: at sentence #270000, processed 6000436 words, keeping 50416 word types\n",
      "2020-04-30 11:17:44,401: INFO : PROGRESS: at sentence #280000, processed 6226315 words, keeping 51640 word types\n",
      "2020-04-30 11:17:44,704: INFO : PROGRESS: at sentence #290000, processed 6449475 words, keeping 52754 word types\n",
      "2020-04-30 11:17:44,949: INFO : PROGRESS: at sentence #300000, processed 6674078 words, keeping 53755 word types\n",
      "2020-04-30 11:17:45,949: INFO : PROGRESS: at sentence #310000, processed 6899392 words, keeping 54734 word types\n",
      "2020-04-30 11:17:46,307: INFO : PROGRESS: at sentence #320000, processed 7124279 words, keeping 55770 word types\n",
      "2020-04-30 11:17:46,640: INFO : PROGRESS: at sentence #330000, processed 7346022 words, keeping 56687 word types\n",
      "2020-04-30 11:17:46,935: INFO : PROGRESS: at sentence #340000, processed 7575534 words, keeping 57629 word types\n",
      "2020-04-30 11:17:47,187: INFO : PROGRESS: at sentence #350000, processed 7798804 words, keeping 58485 word types\n",
      "2020-04-30 11:17:47,456: INFO : PROGRESS: at sentence #360000, processed 8019467 words, keeping 59345 word types\n",
      "2020-04-30 11:17:47,738: INFO : PROGRESS: at sentence #370000, processed 8246659 words, keeping 60161 word types\n",
      "2020-04-30 11:17:48,020: INFO : PROGRESS: at sentence #380000, processed 8471806 words, keeping 61069 word types\n",
      "2020-04-30 11:17:48,273: INFO : PROGRESS: at sentence #390000, processed 8701556 words, keeping 61810 word types\n",
      "2020-04-30 11:17:48,498: INFO : PROGRESS: at sentence #400000, processed 8924505 words, keeping 62546 word types\n",
      "2020-04-30 11:17:48,925: INFO : PROGRESS: at sentence #410000, processed 9145855 words, keeping 63263 word types\n",
      "2020-04-30 11:17:49,175: INFO : PROGRESS: at sentence #420000, processed 9366935 words, keeping 64024 word types\n",
      "2020-04-30 11:17:49,393: INFO : PROGRESS: at sentence #430000, processed 9594472 words, keeping 64795 word types\n",
      "2020-04-30 11:17:49,594: INFO : PROGRESS: at sentence #440000, processed 9821225 words, keeping 65539 word types\n",
      "2020-04-30 11:17:49,819: INFO : PROGRESS: at sentence #450000, processed 10044987 words, keeping 66378 word types\n",
      "2020-04-30 11:17:50,055: INFO : PROGRESS: at sentence #460000, processed 10277747 words, keeping 67158 word types\n",
      "2020-04-30 11:17:50,326: INFO : PROGRESS: at sentence #470000, processed 10505672 words, keeping 67775 word types\n",
      "2020-04-30 11:17:50,536: INFO : PROGRESS: at sentence #480000, processed 10726056 words, keeping 68500 word types\n",
      "2020-04-30 11:17:50,761: INFO : PROGRESS: at sentence #490000, processed 10952800 words, keeping 69256 word types\n",
      "2020-04-30 11:17:51,015: INFO : PROGRESS: at sentence #500000, processed 11174456 words, keeping 69892 word types\n",
      "2020-04-30 11:17:51,282: INFO : PROGRESS: at sentence #510000, processed 11399731 words, keeping 70593 word types\n",
      "2020-04-30 11:17:51,527: INFO : PROGRESS: at sentence #520000, processed 11623082 words, keeping 71267 word types\n",
      "2020-04-30 11:17:51,840: INFO : PROGRESS: at sentence #530000, processed 11847480 words, keeping 71877 word types\n",
      "2020-04-30 11:17:52,201: INFO : PROGRESS: at sentence #540000, processed 12072095 words, keeping 72537 word types\n",
      "2020-04-30 11:17:52,507: INFO : PROGRESS: at sentence #550000, processed 12297646 words, keeping 73212 word types\n",
      "2020-04-30 11:17:52,785: INFO : PROGRESS: at sentence #560000, processed 12518936 words, keeping 73861 word types\n",
      "2020-04-30 11:17:53,361: INFO : PROGRESS: at sentence #570000, processed 12748083 words, keeping 74431 word types\n",
      "2020-04-30 11:17:53,663: INFO : PROGRESS: at sentence #580000, processed 12969579 words, keeping 75087 word types\n",
      "2020-04-30 11:17:54,174: INFO : PROGRESS: at sentence #590000, processed 13195103 words, keeping 75734 word types\n",
      "2020-04-30 11:17:55,892: INFO : PROGRESS: at sentence #600000, processed 13417301 words, keeping 76295 word types\n",
      "2020-04-30 11:17:56,274: INFO : PROGRESS: at sentence #610000, processed 13638324 words, keeping 76953 word types\n",
      "2020-04-30 11:17:56,505: INFO : PROGRESS: at sentence #620000, processed 13864649 words, keeping 77504 word types\n",
      "2020-04-30 11:17:56,729: INFO : PROGRESS: at sentence #630000, processed 14088935 words, keeping 78067 word types\n",
      "2020-04-30 11:17:57,026: INFO : PROGRESS: at sentence #640000, processed 14309718 words, keeping 78693 word types\n",
      "2020-04-30 11:17:57,304: INFO : PROGRESS: at sentence #650000, processed 14535474 words, keeping 79296 word types\n",
      "2020-04-30 11:17:57,572: INFO : PROGRESS: at sentence #660000, processed 14758264 words, keeping 79865 word types\n",
      "2020-04-30 11:17:57,783: INFO : PROGRESS: at sentence #670000, processed 14981657 words, keeping 80382 word types\n",
      "2020-04-30 11:17:58,011: INFO : PROGRESS: at sentence #680000, processed 15206489 words, keeping 80913 word types\n",
      "2020-04-30 11:17:58,157: INFO : PROGRESS: at sentence #690000, processed 15428682 words, keeping 81483 word types\n",
      "2020-04-30 11:17:58,306: INFO : PROGRESS: at sentence #700000, processed 15657388 words, keeping 82075 word types\n",
      "2020-04-30 11:17:58,462: INFO : PROGRESS: at sentence #710000, processed 15880377 words, keeping 82561 word types\n",
      "2020-04-30 11:17:58,639: INFO : PROGRESS: at sentence #720000, processed 16105664 words, keeping 83037 word types\n",
      "2020-04-30 11:17:59,015: INFO : PROGRESS: at sentence #730000, processed 16332045 words, keeping 83572 word types\n",
      "2020-04-30 11:17:59,798: INFO : PROGRESS: at sentence #740000, processed 16553078 words, keeping 84128 word types\n",
      "2020-04-30 11:18:01,349: INFO : PROGRESS: at sentence #750000, processed 16771405 words, keeping 84600 word types\n",
      "2020-04-30 11:18:01,910: INFO : PROGRESS: at sentence #760000, processed 16990809 words, keeping 85069 word types\n",
      "2020-04-30 11:18:02,162: INFO : PROGRESS: at sentence #770000, processed 17217946 words, keeping 85645 word types\n",
      "2020-04-30 11:18:02,372: INFO : PROGRESS: at sentence #780000, processed 17448092 words, keeping 86161 word types\n",
      "2020-04-30 11:18:02,572: INFO : PROGRESS: at sentence #790000, processed 17675168 words, keeping 86666 word types\n",
      "2020-04-30 11:18:02,736: INFO : collected 86997 word types from a corpus of 17798269 raw words and 795538 sentences\n",
      "2020-04-30 11:18:02,743: INFO : Loading a fresh vocabulary\n",
      "2020-04-30 11:18:39,249: INFO : effective_min_count=40 retains 11986 unique words (13% of original 86997, drops 75011)\n",
      "2020-04-30 11:18:39,273: INFO : effective_min_count=40 leaves 17434031 word corpus (97% of original 17798269, drops 364238)\n",
      "2020-04-30 11:18:39,368: INFO : deleting the raw counts dictionary of 86997 items\n",
      "2020-04-30 11:18:39,373: INFO : sample=0.001 downsamples 50 most-common words\n",
      "2020-04-30 11:18:39,374: INFO : downsampling leaves estimated 12872362 word corpus (73.8% of prior 17434031)\n",
      "2020-04-30 11:18:39,441: INFO : estimated required memory for 11986 words and 300 dimensions: 34759400 bytes\n",
      "2020-04-30 11:18:39,442: INFO : resetting layer weights\n",
      "2020-04-30 11:18:43,922: INFO : training model with 4 workers on 11986 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-04-30 11:18:44,996: INFO : EPOCH 1 - PROGRESS: at 3.25% examples, 417092 words/s, in_qsize 8, out_qsize 2\n",
      "2020-04-30 11:18:46,027: INFO : EPOCH 1 - PROGRESS: at 5.71% examples, 361555 words/s, in_qsize 6, out_qsize 3\n",
      "2020-04-30 11:18:47,035: INFO : EPOCH 1 - PROGRESS: at 8.45% examples, 355690 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:18:48,037: INFO : EPOCH 1 - PROGRESS: at 12.18% examples, 385490 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:18:49,062: INFO : EPOCH 1 - PROGRESS: at 15.01% examples, 378844 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:18:50,105: INFO : EPOCH 1 - PROGRESS: at 18.30% examples, 382673 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:18:51,108: INFO : EPOCH 1 - PROGRESS: at 22.15% examples, 397788 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:18:52,138: INFO : EPOCH 1 - PROGRESS: at 25.91% examples, 406843 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:18:53,144: INFO : EPOCH 1 - PROGRESS: at 29.41% examples, 411752 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:18:54,151: INFO : EPOCH 1 - PROGRESS: at 32.75% examples, 412257 words/s, in_qsize 6, out_qsize 2\n",
      "2020-04-30 11:18:55,160: INFO : EPOCH 1 - PROGRESS: at 36.06% examples, 413008 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:18:56,161: INFO : EPOCH 1 - PROGRESS: at 39.75% examples, 418189 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:18:57,168: INFO : EPOCH 1 - PROGRESS: at 42.08% examples, 409238 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:18:58,227: INFO : EPOCH 1 - PROGRESS: at 44.56% examples, 401107 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:18:59,236: INFO : EPOCH 1 - PROGRESS: at 47.79% examples, 401949 words/s, in_qsize 7, out_qsize 1\n",
      "2020-04-30 11:19:00,280: INFO : EPOCH 1 - PROGRESS: at 50.79% examples, 400141 words/s, in_qsize 5, out_qsize 2\n",
      "2020-04-30 11:19:01,315: INFO : EPOCH 1 - PROGRESS: at 52.38% examples, 387900 words/s, in_qsize 4, out_qsize 3\n",
      "2020-04-30 11:19:02,317: INFO : EPOCH 1 - PROGRESS: at 55.32% examples, 387584 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:03,348: INFO : EPOCH 1 - PROGRESS: at 57.47% examples, 381449 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:04,378: INFO : EPOCH 1 - PROGRESS: at 60.06% examples, 378826 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:05,397: INFO : EPOCH 1 - PROGRESS: at 63.39% examples, 380697 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:19:06,420: INFO : EPOCH 1 - PROGRESS: at 66.90% examples, 383612 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:07,430: INFO : EPOCH 1 - PROGRESS: at 70.93% examples, 389248 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:08,431: INFO : EPOCH 1 - PROGRESS: at 75.02% examples, 394867 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:19:09,748: INFO : EPOCH 1 - PROGRESS: at 77.72% examples, 388147 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:19:10,820: INFO : EPOCH 1 - PROGRESS: at 78.05% examples, 374261 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:11,833: INFO : EPOCH 1 - PROGRESS: at 79.52% examples, 367363 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:12,839: INFO : EPOCH 1 - PROGRESS: at 82.99% examples, 370073 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:13,845: INFO : EPOCH 1 - PROGRESS: at 86.81% examples, 374059 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:14,858: INFO : EPOCH 1 - PROGRESS: at 90.77% examples, 378376 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:15,902: INFO : EPOCH 1 - PROGRESS: at 94.89% examples, 382511 words/s, in_qsize 8, out_qsize 2\n",
      "2020-04-30 11:19:16,917: INFO : EPOCH 1 - PROGRESS: at 97.78% examples, 382131 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:17,416: INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-30 11:19:17,421: INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-30 11:19:17,422: INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-30 11:19:17,432: INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-30 11:19:17,433: INFO : EPOCH - 1 : training on 17798269 raw words (12870132 effective words) took 33.4s, 384845 effective words/s\n",
      "2020-04-30 11:19:18,504: INFO : EPOCH 2 - PROGRESS: at 2.12% examples, 270738 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:19,516: INFO : EPOCH 2 - PROGRESS: at 5.43% examples, 346007 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:19:20,550: INFO : EPOCH 2 - PROGRESS: at 7.77% examples, 325700 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:21,578: INFO : EPOCH 2 - PROGRESS: at 10.98% examples, 344516 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:22,599: INFO : EPOCH 2 - PROGRESS: at 13.88% examples, 347833 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:23,608: INFO : EPOCH 2 - PROGRESS: at 16.17% examples, 338914 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:24,642: INFO : EPOCH 2 - PROGRESS: at 19.71% examples, 352368 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:25,674: INFO : EPOCH 2 - PROGRESS: at 23.27% examples, 363556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:26,679: INFO : EPOCH 2 - PROGRESS: at 25.51% examples, 355261 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:27,690: INFO : EPOCH 2 - PROGRESS: at 28.64% examples, 359688 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:28,690: INFO : EPOCH 2 - PROGRESS: at 32.51% examples, 371355 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:29,706: INFO : EPOCH 2 - PROGRESS: at 34.70% examples, 363481 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:30,711: INFO : EPOCH 2 - PROGRESS: at 37.73% examples, 365347 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:31,721: INFO : EPOCH 2 - PROGRESS: at 41.76% examples, 375937 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:32,763: INFO : EPOCH 2 - PROGRESS: at 44.61% examples, 374430 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:33,768: INFO : EPOCH 2 - PROGRESS: at 46.67% examples, 367705 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:34,767: INFO : EPOCH 2 - PROGRESS: at 50.00% examples, 371499 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:19:35,775: INFO : EPOCH 2 - PROGRESS: at 53.72% examples, 377095 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:36,783: INFO : EPOCH 2 - PROGRESS: at 56.71% examples, 377535 words/s, in_qsize 7, out_qsize 1\n",
      "2020-04-30 11:19:37,790: INFO : EPOCH 2 - PROGRESS: at 60.06% examples, 380523 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:38,843: INFO : EPOCH 2 - PROGRESS: at 63.28% examples, 381046 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:19:39,868: INFO : EPOCH 2 - PROGRESS: at 66.12% examples, 380045 words/s, in_qsize 7, out_qsize 2\n",
      "2020-04-30 11:19:40,875: INFO : EPOCH 2 - PROGRESS: at 70.50% examples, 387774 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:41,881: INFO : EPOCH 2 - PROGRESS: at 74.34% examples, 392227 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:42,893: INFO : EPOCH 2 - PROGRESS: at 78.55% examples, 397914 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:19:43,927: INFO : EPOCH 2 - PROGRESS: at 82.15% examples, 399805 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:44,957: INFO : EPOCH 2 - PROGRESS: at 85.28% examples, 399540 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:45,967: INFO : EPOCH 2 - PROGRESS: at 88.02% examples, 397797 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:46,967: INFO : EPOCH 2 - PROGRESS: at 91.49% examples, 399463 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:19:47,973: INFO : EPOCH 2 - PROGRESS: at 95.65% examples, 403585 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:19:48,992: INFO : EPOCH 2 - PROGRESS: at 98.77% examples, 403595 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:19:49,312: INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-30 11:19:49,318: INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-30 11:19:49,325: INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-30 11:19:49,349: INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-30 11:19:49,350: INFO : EPOCH - 2 : training on 17798269 raw words (12871632 effective words) took 31.9s, 404018 effective words/s\n",
      "2020-04-30 11:19:50,452: INFO : EPOCH 3 - PROGRESS: at 2.55% examples, 325812 words/s, in_qsize 7, out_qsize 1\n",
      "2020-04-30 11:19:51,461: INFO : EPOCH 3 - PROGRESS: at 5.92% examples, 377197 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:52,464: INFO : EPOCH 3 - PROGRESS: at 10.09% examples, 426158 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:53,491: INFO : EPOCH 3 - PROGRESS: at 12.39% examples, 391504 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:54,505: INFO : EPOCH 3 - PROGRESS: at 15.68% examples, 395782 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:55,519: INFO : EPOCH 3 - PROGRESS: at 19.15% examples, 402131 words/s, in_qsize 7, out_qsize 2\n",
      "2020-04-30 11:19:56,533: INFO : EPOCH 3 - PROGRESS: at 22.70% examples, 408902 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:57,549: INFO : EPOCH 3 - PROGRESS: at 26.92% examples, 424326 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:19:58,560: INFO : EPOCH 3 - PROGRESS: at 30.61% examples, 429511 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:19:59,568: INFO : EPOCH 3 - PROGRESS: at 34.10% examples, 430273 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:00,583: INFO : EPOCH 3 - PROGRESS: at 37.11% examples, 426038 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:20:01,593: INFO : EPOCH 3 - PROGRESS: at 41.42% examples, 436334 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:02,596: INFO : EPOCH 3 - PROGRESS: at 43.98% examples, 428288 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:03,636: INFO : EPOCH 3 - PROGRESS: at 46.22% examples, 417248 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:04,851: INFO : EPOCH 3 - PROGRESS: at 48.87% examples, 406876 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:05,862: INFO : EPOCH 3 - PROGRESS: at 50.45% examples, 394136 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:06,885: INFO : EPOCH 3 - PROGRESS: at 54.04% examples, 397521 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:20:07,898: INFO : EPOCH 3 - PROGRESS: at 57.90% examples, 403028 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:20:08,943: INFO : EPOCH 3 - PROGRESS: at 61.42% examples, 404775 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:09,949: INFO : EPOCH 3 - PROGRESS: at 64.50% examples, 404299 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:10,984: INFO : EPOCH 3 - PROGRESS: at 67.69% examples, 403970 words/s, in_qsize 8, out_qsize 3\n",
      "2020-04-30 11:20:11,989: INFO : EPOCH 3 - PROGRESS: at 71.22% examples, 406143 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:12,992: INFO : EPOCH 3 - PROGRESS: at 75.30% examples, 411254 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:14,207: INFO : EPOCH 3 - PROGRESS: at 77.66% examples, 403299 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:20:15,324: INFO : EPOCH 3 - PROGRESS: at 79.28% examples, 394000 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:16,339: INFO : EPOCH 3 - PROGRESS: at 82.27% examples, 393333 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:17,363: INFO : EPOCH 3 - PROGRESS: at 85.68% examples, 394678 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:18,366: INFO : EPOCH 3 - PROGRESS: at 89.53% examples, 398212 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:19,374: INFO : EPOCH 3 - PROGRESS: at 92.95% examples, 399527 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:20,381: INFO : EPOCH 3 - PROGRESS: at 96.69% examples, 401919 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:21,093: INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-30 11:20:21,102: INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-30 11:20:21,112: INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-30 11:20:21,133: INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-30 11:20:21,134: INFO : EPOCH - 3 : training on 17798269 raw words (12871656 effective words) took 31.7s, 406010 effective words/s\n",
      "2020-04-30 11:20:22,213: INFO : EPOCH 4 - PROGRESS: at 3.36% examples, 429293 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:23,216: INFO : EPOCH 4 - PROGRESS: at 7.48% examples, 476737 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:24,234: INFO : EPOCH 4 - PROGRESS: at 11.20% examples, 473951 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:20:25,256: INFO : EPOCH 4 - PROGRESS: at 14.67% examples, 463363 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:26,339: INFO : EPOCH 4 - PROGRESS: at 16.89% examples, 420421 words/s, in_qsize 6, out_qsize 2\n",
      "2020-04-30 11:20:27,349: INFO : EPOCH 4 - PROGRESS: at 19.04% examples, 395894 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:28,352: INFO : EPOCH 4 - PROGRESS: at 23.09% examples, 413088 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:29,388: INFO : EPOCH 4 - PROGRESS: at 26.46% examples, 413692 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:20:30,464: INFO : EPOCH 4 - PROGRESS: at 28.08% examples, 388169 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:31,491: INFO : EPOCH 4 - PROGRESS: at 30.74% examples, 382426 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:32,512: INFO : EPOCH 4 - PROGRESS: at 34.39% examples, 388726 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:33,551: INFO : EPOCH 4 - PROGRESS: at 37.79% examples, 391634 words/s, in_qsize 7, out_qsize 2\n",
      "2020-04-30 11:20:34,561: INFO : EPOCH 4 - PROGRESS: at 41.87% examples, 401497 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:20:35,572: INFO : EPOCH 4 - PROGRESS: at 45.60% examples, 406880 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:36,609: INFO : EPOCH 4 - PROGRESS: at 49.10% examples, 409044 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:37,618: INFO : EPOCH 4 - PROGRESS: at 51.59% examples, 403225 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:38,637: INFO : EPOCH 4 - PROGRESS: at 55.26% examples, 406971 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:20:39,640: INFO : EPOCH 4 - PROGRESS: at 58.84% examples, 410274 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:40,709: INFO : EPOCH 4 - PROGRESS: at 61.69% examples, 406700 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:20:41,723: INFO : EPOCH 4 - PROGRESS: at 64.95% examples, 407010 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:42,745: INFO : EPOCH 4 - PROGRESS: at 67.12% examples, 400760 words/s, in_qsize 7, out_qsize 1\n",
      "2020-04-30 11:20:43,761: INFO : EPOCH 4 - PROGRESS: at 69.70% examples, 397425 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:44,769: INFO : EPOCH 4 - PROGRESS: at 72.60% examples, 396393 words/s, in_qsize 6, out_qsize 3\n",
      "2020-04-30 11:20:45,794: INFO : EPOCH 4 - PROGRESS: at 75.53% examples, 395166 words/s, in_qsize 8, out_qsize 2\n",
      "2020-04-30 11:20:46,804: INFO : EPOCH 4 - PROGRESS: at 78.89% examples, 396462 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:47,821: INFO : EPOCH 4 - PROGRESS: at 82.38% examples, 398118 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:48,846: INFO : EPOCH 4 - PROGRESS: at 86.13% examples, 400839 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:49,853: INFO : EPOCH 4 - PROGRESS: at 90.07% examples, 404660 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-30 11:20:50,853: INFO : EPOCH 4 - PROGRESS: at 93.80% examples, 407071 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:51,857: INFO : EPOCH 4 - PROGRESS: at 97.29% examples, 408338 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:52,485: INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-30 11:20:52,501: INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-30 11:20:52,504: INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-30 11:20:52,526: INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-30 11:20:52,529: INFO : EPOCH - 4 : training on 17798269 raw words (12869954 effective words) took 31.3s, 410851 effective words/s\n",
      "2020-04-30 11:20:53,584: INFO : EPOCH 5 - PROGRESS: at 2.68% examples, 343281 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:54,591: INFO : EPOCH 5 - PROGRESS: at 6.62% examples, 421840 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:55,623: INFO : EPOCH 5 - PROGRESS: at 10.15% examples, 425849 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:20:56,649: INFO : EPOCH 5 - PROGRESS: at 13.36% examples, 419929 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:57,663: INFO : EPOCH 5 - PROGRESS: at 15.45% examples, 388735 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:58,678: INFO : EPOCH 5 - PROGRESS: at 18.02% examples, 377226 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:20:59,690: INFO : EPOCH 5 - PROGRESS: at 21.98% examples, 394664 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:21:00,692: INFO : EPOCH 5 - PROGRESS: at 24.67% examples, 388678 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:01,701: INFO : EPOCH 5 - PROGRESS: at 27.80% examples, 389951 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:02,713: INFO : EPOCH 5 - PROGRESS: at 32.07% examples, 404352 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:03,727: INFO : EPOCH 5 - PROGRESS: at 35.33% examples, 405101 words/s, in_qsize 7, out_qsize 1\n",
      "2020-04-30 11:21:04,735: INFO : EPOCH 5 - PROGRESS: at 38.34% examples, 403588 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:21:05,749: INFO : EPOCH 5 - PROGRESS: at 42.25% examples, 410935 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:21:06,751: INFO : EPOCH 5 - PROGRESS: at 45.72% examples, 413472 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:07,787: INFO : EPOCH 5 - PROGRESS: at 49.33% examples, 416163 words/s, in_qsize 6, out_qsize 3\n",
      "2020-04-30 11:21:08,825: INFO : EPOCH 5 - PROGRESS: at 52.43% examples, 413998 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:09,835: INFO : EPOCH 5 - PROGRESS: at 55.66% examples, 414031 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:21:10,856: INFO : EPOCH 5 - PROGRESS: at 59.34% examples, 417381 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:11,859: INFO : EPOCH 5 - PROGRESS: at 62.25% examples, 415122 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:12,893: INFO : EPOCH 5 - PROGRESS: at 65.84% examples, 416775 words/s, in_qsize 8, out_qsize 2\n",
      "2020-04-30 11:21:13,902: INFO : EPOCH 5 - PROGRESS: at 69.36% examples, 418373 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:21:14,910: INFO : EPOCH 5 - PROGRESS: at 72.44% examples, 417297 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:15,972: INFO : EPOCH 5 - PROGRESS: at 75.24% examples, 413794 words/s, in_qsize 8, out_qsize 3\n",
      "2020-04-30 11:21:16,996: INFO : EPOCH 5 - PROGRESS: at 78.10% examples, 411490 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:18,021: INFO : EPOCH 5 - PROGRESS: at 81.70% examples, 413069 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:21:19,058: INFO : EPOCH 5 - PROGRESS: at 85.63% examples, 416007 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:21:20,080: INFO : EPOCH 5 - PROGRESS: at 89.21% examples, 417336 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-30 11:21:21,114: INFO : EPOCH 5 - PROGRESS: at 93.06% examples, 419674 words/s, in_qsize 8, out_qsize 0\n",
      "2020-04-30 11:21:22,118: INFO : EPOCH 5 - PROGRESS: at 96.59% examples, 420561 words/s, in_qsize 6, out_qsize 1\n",
      "2020-04-30 11:21:22,949: INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-30 11:21:22,955: INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-30 11:21:22,966: INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-30 11:21:22,978: INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-30 11:21:22,980: INFO : EPOCH - 5 : training on 17798269 raw words (12872865 effective words) took 30.4s, 423355 effective words/s\n",
      "2020-04-30 11:21:22,984: INFO : training on a 88991345 raw words (64356239 effective words) took 159.1s, 404602 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#Create Word2Vec\n",
    "#parameters\n",
    "num_vector = 300 # 문제 벡처 차원 수\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "windows = 10 #문자열 창 크기 (전후)\n",
    "downsampling = 1e-3 # 0.0001 \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences, \n",
    "    workers= num_workers,\n",
    "    size = num_vector,\n",
    "    min_count = min_word_count,\n",
    "    window = windows,\n",
    "    sample = downsampling\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-30 11:23:40,787: INFO : precomputing L2-norms of word weight vectors\n",
      "2020-04-30 11:23:40,862: INFO : saving Word2Vec object under 300vector_40min_10text, separately None\n",
      "2020-04-30 11:23:40,894: INFO : not storing attribute vectors_norm\n",
      "2020-04-30 11:23:40,943: INFO : not storing attribute cum_table\n",
      "2020-04-30 11:23:41,585: INFO : saved 300vector_40min_10text\n"
     ]
    }
   ],
   "source": [
    "#학습 후 메모리 정리 (unlad)\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"300vector_40min_10text\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-30 11:24:11,287: WARNING : vectors for words {'france', 'germany'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'england'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"france england korea germany tokyo\".split()) #나라인데 도시여서 tokyo 관계없음 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6406874656677246),\n",
       " ('ladi', 0.5394983291625977),\n",
       " ('lad', 0.5073885917663574),\n",
       " ('millionair', 0.5039361119270325),\n",
       " ('businessman', 0.49491316080093384),\n",
       " ('loner', 0.49450647830963135),\n",
       " ('widow', 0.4717475175857544),\n",
       " ('policeman', 0.4698629677295685),\n",
       " ('farmer', 0.4688703715801239),\n",
       " ('men', 0.46664324402809143)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('schooler', 0.7042346596717834),\n",
       " ('colleg', 0.6707702279090881),\n",
       " ('junior', 0.6026756763458252),\n",
       " ('noon', 0.5779948234558105),\n",
       " ('tech', 0.5303436517715454),\n",
       " ('gym', 0.5116730332374573),\n",
       " ('student', 0.4988793432712555),\n",
       " ('graduat', 0.48448991775512695),\n",
       " ('class', 0.48017966747283936),\n",
       " ('classroom', 0.47576698660850525)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movi', 0.8528131246566772),\n",
       " ('flick', 0.6047983169555664),\n",
       " ('documentari', 0.5563231706619263),\n",
       " ('pictur', 0.5506850481033325),\n",
       " ('cinema', 0.5181126594543457),\n",
       " ('masterpiec', 0.4901534616947174),\n",
       " ('it', 0.4856082797050476),\n",
       " ('sequel', 0.4792662262916565),\n",
       " ('effort', 0.46896636486053467),\n",
       " ('thriller', 0.4656416177749634)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"film\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unhappi', 0.4541730582714081),\n",
       " ('bitter', 0.41757822036743164),\n",
       " ('afraid', 0.4127125144004822),\n",
       " ('satisfi', 0.40617939829826355),\n",
       " ('sad', 0.4056433439254761),\n",
       " ('happier', 0.40464216470718384),\n",
       " ('glad', 0.385473370552063),\n",
       " ('upset', 0.38082295656204224),\n",
       " ('proud', 0.3802988529205322),\n",
       " ('lucki', 0.379236102104187)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"happi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 결과 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec으로 벡터화 한 단어를 t-SNE 를 통해 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
